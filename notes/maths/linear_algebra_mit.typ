#let title = [
  Linear Algebra
]
#set page(
  paper: "us-letter",
  numbering: "1",
  columns: 2,
)
#set text(
  font: "New Computer Modern",
  size: 10pt
)
#set par(justify: true)
#place(
  top + center,
  float: true,
  scope: "parent",
  clearance: 2em,
)[
  #align(center, text(17pt)[
    *#title*
  ])
]
#set heading(numbering: "1.")
#set math.mat(delim: "[")
#set math.vec(delim: "[")

= Introduction
Row picture consists of finding the intersection of lines described by rows of the matrix, the equations. Column picture consists of thinking of vectors that combine to get another vecto, taking their linear combination.

$"Row picture:" cases(
  2x -y &= 0,
  -x + 2y &= 3
)$

Matrix picture: 
$mat(2,-1;-1,2)vec(x,y) = vec(0,3)$

Column picture:
$x vec(2,-1) + y vec(-1,2) = vec(0,3)$

As it can be easily seen, $x=1, y=2$ solves this example.

== Elimination
$ mat(1,2,1;3,8,1;0,4,1) arrow mat(1,2,1;0,2,-2;0,4,1) arrow mat(1,2,1;0,2,-2;0,4,1) arrow mat(1,2,1;0,2,-2;0,0,5) $

An upper triangular mtrix was created by systematicly finding the pivots of the matrix. 0 can never be a pivot, if thats the case one must switch rows so that 0 leaves the pivot position.

$ A x = b arrow U x = c $

To find solutions back-substitution is done, find from the last pivot all the way to the top.

Elimination deals with row operations:
$ mat(1,0,0;-3,1,0;0,0,1) mat(1,2,1;3,8,1;0,4,1) = mat(1,2,1;0,2,-2;0,4,1) $
$ mat(1,0,0;0,1,0;0,-2,1) mat(1,2,1;0,2,-2;0,4,1) = mat(1,2,1;0,2,-2;0,0,5) $
$ E_(3,2)(E_(2,1) A) = (E_(3,2)E_(2,1)) A = U $

*Inverses* How to undo, for example, $E_(2,1)$.

$ mat(1,0,0;-3,1,0;0,0,1) mat(1,0,0;3,1,0;0,0,1) = mat(1,0,0;0,1,0;0,0,1) $

== Matrix Multiplication
$ A B = C $
$ c_(i,j) = "row" i "of" A dot "column" j "of" B = sum a_(i k) b_(k j) $
Shapes need to match in order for this operation to be possible: $A_(m times n) B_(n times p) = C_(m times p)$

- Rows of C are linear combinations of rows of B.
- Columns of C are linear combinations of columns of A. 
- A column times a row creates a matrix.
- This multiplication can also be done by blocks using the same algorithms.

== Inverses
Square matrices can be invertable, non-singular. $ A^(-1) A = A A^(-1) = I $
If a square matrix is singular, the matrix doesn't have an inverse. If you can find a vector $x eq.not 0$ with $A x = 0$, the matrix does not have an inverse.

Finding inverses is like solving multiple systems of equations at once as we can easily separate the system $A A^(-1) = I$ into multiple systems by taking columns of $A^(-1)$ and setting them equal to corresponding columns of the identity matrix.

=== Gauss-Jordan
Augment the entire identity matrix instead of a single column. Follow the elimination steps.

$ mat(1,3, 1,0; 2,7,0,1; augment: #2 ) arrow mat(1,3,1,0; 0,1,-2,1; augment: #2 ) arrow mat(1,0, 7,-3; 0,1,-2,1; augment: #2 ) $

$ mat(A, I, augment: #1) arrow mat(I, A^(-1), augment: #1) $

Elimination steps are accumulated through this process. This accumulated matrix multiplied $A$ to create $I$ that means $E$ has to be the inverse of $A$.

$ (A^T)^(-1) = (A^(-1))^T $

== $A=L U$ Factorization
$L$ can be generated by taking the inverses of $E$ elimination matrices and accumulating them on the other side of the equation.

$ E_(3,2)E_(3,1)E_(2,1) A = U $
$ A = E_(2,1)^(-1)E_(3,1)^(-1)E_(3,2)^(-1) U = L U $

If no row exchanges are done, multipliers go directly into $L$ without interference, that's the reason why it is desirable as opposed to $E$.

Elimination is in order $n^3$ as a total of $sum_(k=1)^(n) k^2$

=== Permutation Matrices
They form a group that when multiplied with each other produce again permutation matrices, their inverses are still permutation matrices (and they equal to their transposes). These matrices perform row exchanges.
$ P A = L U $
The idea of this factorization is that the matrix $P$ puts the rows in the correct order so that the rest of the operations can be done without requiring further exchanges. (Since multiplication of permutation matrices produec a permutation matix this can be written as a single matrix)

=== Transposes
$ (A^T)_(i j) = A_(j i) $

Symmetric matrices are the matrices that equal to their transposes.

$R^T R$ is always symmetric. Why? Take the transpose!

$(R^T R)^T = R^T R^(T T) =  R^T R$


